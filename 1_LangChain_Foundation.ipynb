{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit 2 - Part 1a: LangChain Setup & Models\n",
    "\n",
    "## 1. Introduction: Why LangChain?\n",
    "\n",
    "Before we write code, you might ask: **\"Why not just use the Gemini API directly?\"**\n",
    "\n",
    "Imagine you build an app using OpenAI. Six months later, you want to switch to Gemini because it's cheaper. \n",
    "- **Without LangChain:** You have to rewrite all your API calls (different endpoint, different parameters, different response format).\n",
    "- **With LangChain:** You change **one line of code**.\n",
    "\n",
    "LangChain is a **framework** that provides a standard interface for any Language Model. It's like a universal adapter for AI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Concept: Tokens (The Atom of AI)\n",
    "\n",
    "Models don't read words. They read **Tokens**.\n",
    "A token can be a word, part of a word, or even a space.\n",
    "\n",
    "**Example:**\n",
    "- Text: `\"Hello World\"`\n",
    "- Tokens: `[101, 2055, 309]` (Hypothetical IDs)\n",
    " \n",
    "### Cost & Context\n",
    "You pay per token. The model has a limit on how many tokens it can remember (Context Window)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Setting Up the Environment\n",
    "\n",
    "We need two main libraries:\n",
    "1.  `langchain`: The core logic.\n",
    "2.  `langchain-google-genai`: The specific connector for Google models.\n",
    "\n",
    "Let's install them quietly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1293637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "anaconda-cloud-auth 0.1.4 requires pydantic<2.0, but you have pydantic 2.12.5 which is incompatible.\n",
      "streamlit 1.30.0 requires packaging<24,>=16.8, but you have packaging 26.0 which is incompatible.\n",
      "streamlit 1.30.0 requires tenacity<9,>=8.1.0, but you have tenacity 9.1.4 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "%pip install python-dotenv --upgrade --quiet langchain langchain-google-genai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfba7420",
   "metadata": {},
   "source": [
    "## 4. Securely Loading API Keys\n",
    "\n",
    "Never hardcode your API keys (e.g., `api_key = \"AIzaSy...\"`) in a notebook. If you share the notebook, your key is stolen.\n",
    "\n",
    "**Best Practice:** Use `getpass` to enter it securely every session, or load it from environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "630e6eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "if \"GOOGLE_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google API Key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e55527e",
   "metadata": {},
   "source": [
    "## 5. The Architecture (Flowchart)\n",
    "\n",
    "What happens when we run the code below?\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    User[Your Python Code] -->|API Request (JSON)| Google[Google Gemini Cloud]\n",
    "    Google -->|Inference| Model[Gemini 1.5 Pro]\n",
    "    Model -->|Response| Google\n",
    "    Google -->|API Response (JSON)| User\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c2841b",
   "metadata": {},
   "source": [
    "## 6. The `Temperature` Parameter (Critical Thinking)\n",
    "\n",
    "When we initialize a model, we aren't just \"turning it on\". We are configuring its brain.\n",
    "\n",
    "The most important setting is `temperature`.\n",
    "- **Range:** 0.0 to 1.0 (sometimes higher).\n",
    "- **Meaning:** How \"random\" should the choice of the next word be?\n",
    "\n",
    "Let's create **two** versions of the same model to compare them side-by-side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2910b057",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Model A: The \"Accountant\" (Precision)\n",
    "llm_focused = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0.0)\n",
    "\n",
    "# Model B: The \"Poet\" (Creativity)\n",
    "llm_creative = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404304cd",
   "metadata": {},
   "source": [
    "## 7. Experiment: Consistency vs. Creativity\n",
    "\n",
    "We will ask both models to \"Define the word 'Idea' in one sentence.\"\n",
    "We will run the code **TWICE** for each model.\n",
    "\n",
    "**Hypothesis:**\n",
    "- The Focused model (Temp=0) should say the *exact same thing* both times.\n",
    "- The Creative model (Temp=1) should say *different things*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0de67033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- FOCUSED (Temp=0) ---\n",
      "Run 1: An idea is a thought, concept, or suggestion that is formed or exists in the mind.\n",
      "Run 2: An idea is a thought, concept, or mental image formed in the mind.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Define the word 'Idea' in one sentence.\"\n",
    "\n",
    "print(\"--- FOCUSED (Temp=0) ---\")\n",
    "print(f\"Run 1: {llm_focused.invoke(prompt).content}\")\n",
    "print(f\"Run 2: {llm_focused.invoke(prompt).content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "482e96e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- CREATIVE (Temp=1) ---\n",
      "Run 1: An idea is a concept, thought, or understanding that is formed in the mind.\n",
      "Run 2: An idea is a thought or suggestion that originates in the mind, often serving as a plan, solution, or a new way of understanding something.\n"
     ]
    }
   ],
   "source": [
    "print(\"--- CREATIVE (Temp=1) ---\")\n",
    "print(f\"Run 1: {llm_creative.invoke(prompt).content}\")\n",
    "print(f\"Run 2: {llm_creative.invoke(prompt).content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c155e9",
   "metadata": {},
   "source": [
    "## 8. Conclusion for Part 1a\n",
    "\n",
    "**What did we learn?**\n",
    "1.  **LangChain** abstracts the messy API details.\n",
    "2.  **Tokens** are the currency of AI.\n",
    "3.  **Temperature** is a control knob for randomness.\n",
    "\n",
    "In the next notebook (**1b**), we will look at how to control the *Input* using Prompt Templates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6f6b24",
   "metadata": {},
   "source": [
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d52b42e",
   "metadata": {},
   "source": [
    "# Unit 2 - Part 1b: Prompts & Parsers\n",
    "\n",
    "## 1. Introduction: The Pipeline\n",
    "\n",
    "Real AI apps are not just `print(llm.invoke(\"hi\"))`. They are pipelines.\n",
    "\n",
    "### The Data Flow (Flowchart)\n",
    "```mermaid\n",
    "graph TD\n",
    "    User[User Input 'Bob'] -->|Fill Template| Prompt[Prompt Object]\n",
    "    Prompt -->|List of Messages| Model[LLM]\n",
    "    Model -->|AIMessage Object| Parser[Output Parser]\n",
    "    Parser -->|Clean String| Final['Hello Bob!']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5df5576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup from Part 1a (Hidden for brevity)\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import getpass\n",
    "import os\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "if \"GOOGLE_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google API Key: \")\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2886b0ff",
   "metadata": {},
   "source": [
    "## 2. Strings vs. Messages (Critical Thinking)\n",
    "\n",
    "Most people start by talking to the AI like a human:\n",
    "`llm.invoke(\"Translate this to French: Hello\")`\n",
    "\n",
    "But LLMs understand **Roles**:\n",
    "- **System:** God-mode instructions. (e.g., \"You are a calculator.\")\n",
    "- **Human:** The user.\n",
    "- **AI:** The assistant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a76c687f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ugh, like, seriously? Everyone knows that. It's Paris. Duh. Are you, like, five?\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "\n",
    "# Scenario: Make the AI rude.\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a rude teenager. You use slang and don't care about grammar.\"),\n",
    "    HumanMessage(content=\"What is the capital of France?\")\n",
    "]\n",
    "\n",
    "response = llm.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d490985a",
   "metadata": {},
   "source": [
    "### Why System Messages matter?\n",
    "If you just asked \"What is the capital of France?\" without the System Message, you'd get \"Paris\".\n",
    "The System Message gives you **Control** over the personality and constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8895157b",
   "metadata": {},
   "source": [
    "## 3. The Context Window Concept\n",
    "\n",
    "You might ask: \"Can't I just paste a whole book into the System Message?\"\n",
    "\n",
    "Maybe. \n",
    "Every model has a **Context Window** (e.g., 128k tokens for Gemini Flash).\n",
    "- If you exceed it, the model **forgets the beginning**.\n",
    "- It's like a sliding window over the conversation history."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e6172e",
   "metadata": {},
   "source": [
    "## 4. Prompt Templates: The Safe Way\n",
    "\n",
    "Don't do THIS:\n",
    "`prompt = f\"Translate {user_input} to Spanish\"`\n",
    "\n",
    "Do THIS:\n",
    "`ChatPromptTemplate`.\n",
    "\n",
    "It handles messy input (like quotes or newlines) safely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e5856bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Required variables: ['input_language', 'output_language', 'text']\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a translator. Translate {input_language} to {output_language}.\"),\n",
    "    (\"human\", \"{text}\")\n",
    "])\n",
    "\n",
    "# We can check what inputs it expects\n",
    "print(f\"Required variables: {template.input_variables}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd954d77",
   "metadata": {},
   "source": [
    "## 5. Output Parsers\n",
    "\n",
    "Look at the output of `llm.invoke()`. It's an `AIMessage(content=\"...\")`. \n",
    "Usually, we just want the string inside.\n",
    "\n",
    "**StrOutputParser** extracts just the text via regex or logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b48f0fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Type: <class 'langchain_core.messages.ai.AIMessage'>\n",
      "Parsed Type: <class 'langchain_core.messages.base.TextAccessor'>\n",
      "Content: Hi there! How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# Raw Message\n",
    "raw_msg = llm.invoke(\"Hi\")\n",
    "print(f\"Raw Type: {type(raw_msg)}\")\n",
    "\n",
    "# Parsed String\n",
    "clean_text = parser.invoke(raw_msg)\n",
    "print(f\"Parsed Type: {type(clean_text)}\")\n",
    "print(f\"Content: {clean_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2661ffd1",
   "metadata": {},
   "source": [
    "## 6. Conclusion for Part 1b\n",
    "\n",
    "We have the ingredients:\n",
    "- **Model** (The Brain)\n",
    "- **Prompt Template** (The Input Formatter)\n",
    "- **Parser** (The Output Formatter)\n",
    "\n",
    "In Part **1c**, we will chain them all together using **LCEL**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a638a9f",
   "metadata": {},
   "source": [
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de149e98",
   "metadata": {},
   "source": [
    "# Unit 2 - Part 1c: LCEL (LangChain Expression Language)\n",
    "\n",
    "## 1. Introduction: The Spaghetti Code Problem\n",
    "\n",
    "In software engineering, doing things manually is often easy at first, but messy later. \n",
    "\n",
    "We want to build a pipeline:\n",
    "`Input -> Prompt -> Model -> Parser -> Output`\n",
    "\n",
    "### Visualizing the Chain\n",
    "```mermaid\n",
    "graph LR\n",
    "    Input({\"topic\": \"Crows\"}) -->|Inject| Template[ChatPromptTemplate]\n",
    "    Template -->|Messages| LLM[Gemini Model]\n",
    "    LLM -->|Message| Parser[StrOutputParser]\n",
    "    Parser -->|String| Final[\"Crows are smart...\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "016ca9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup (Hidden)\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import getpass\n",
    "import os\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "if \"GOOGLE_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google API Key: \")\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")\n",
    "template = ChatPromptTemplate.from_template(\"Tell me a fun fact about {topic}.\")\n",
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e1f4ca",
   "metadata": {},
   "source": [
    "## 2. Method A: The Manual Way (Bad)\n",
    "\n",
    "We call each step one by one. This is verbose and hard to modify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c652c803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a fun one:\n",
      "\n",
      "Crows are incredibly intelligent and can actually **recognize individual human faces!** Even more amazing, they can remember those faces for years, and even teach other crows which humans are friendly or threatening. So, if you're ever mean to a crow, don't be surprised if its friends and family give you the stink eye!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "prompt_value = template.invoke({\"topic\": \"Crows\"})\n",
    "\n",
    "time.sleep(35)   # wait > retryDelay mentioned in error\n",
    "\n",
    "response_obj = llm.invoke(prompt_value)\n",
    "\n",
    "final_text = parser.invoke(response_obj)\n",
    "print(final_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159f71f2",
   "metadata": {},
   "source": [
    "## 3. Method B: The LCEL Way (Good)\n",
    "\n",
    "We use the **Pipe Operator (`|`)**. \n",
    "It works just like Unix pipes: pass the output of the left side to the input of the right side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f5c3f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a fun one:\n",
      "\n",
      "Octopuses have **three hearts** and **blue blood**! Two hearts pump blood through their gills, and one larger heart circulates it to the rest of their body. Their blood is blue because it uses a copper-based protein called hemocyanin to carry oxygen, instead of the iron-based hemoglobin that makes our blood red.\n"
     ]
    }
   ],
   "source": [
    "# Define the chain once\n",
    "chain = template | llm | parser\n",
    "\n",
    "# Invoke the whole chain\n",
    "print(chain.invoke({\"topic\": \"Octopuses\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10164fd3",
   "metadata": {},
   "source": [
    "## 4. Why is this \"Critical\"? (Composability)\n",
    "\n",
    "Imagine you want to swap the Model. \n",
    "- **Manual:** You hunt for the line where `llm.invoke` happens.\n",
    "- **LCEL:** You just change the `llm` variable in the chain definition.\n",
    "\n",
    "Imagine you want to add a step (e.g., a spellchecker) between the prompt and the model.\n",
    "- **LCEL:** `chain = template | spellchecker | llm | parser`\n",
    "\n",
    "It makes your AI logic **Composable**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749e720a",
   "metadata": {},
   "source": [
    "## Assignment\n",
    "\n",
    "Create a chain that:\n",
    "1.  Takes a movie name.\n",
    "2.  Asks for its release year.\n",
    "3.  Calculates how many years ago that was (You can try just asking the LLM to do the math).\n",
    "\n",
    "Try to do it in **one line of LCEL**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b591462b",
   "metadata": {},
   "source": [
    "ASSIGNMENT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c7683b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\gowth\\anaconda3\\lib\\site-packages (1.2.10)\n",
      "Requirement already satisfied: langchain-core in c:\\users\\gowth\\anaconda3\\lib\\site-packages (1.2.13)\n",
      "Requirement already satisfied: langchain-google-genai in c:\\users\\gowth\\anaconda3\\lib\\site-packages (4.2.0)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\gowth\\anaconda3\\lib\\site-packages (1.2.1)\n",
      "Requirement already satisfied: langgraph<1.1.0,>=1.0.8 in c:\\users\\gowth\\anaconda3\\lib\\site-packages (from langchain) (1.0.8)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\gowth\\anaconda3\\lib\\site-packages (from langchain) (2.12.5)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in c:\\users\\gowth\\anaconda3\\lib\\site-packages (from langchain-core) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in c:\\users\\gowth\\anaconda3\\lib\\site-packages (from langchain-core) (0.7.3)\n",
      "Requirement already satisfied: packaging>=23.2.0 in c:\\users\\gowth\\anaconda3\\lib\\site-packages (from langchain-core) (26.0)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in c:\\users\\gowth\\anaconda3\\lib\\site-packages (from langchain-core) (6.0.1)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\gowth\\anaconda3\\lib\\site-packages (from langchain-core) (9.1.4)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in c:\\users\\gowth\\anaconda3\\lib\\site-packages (from langchain-core) (4.15.0)\n",
      "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in c:\\users\\gowth\\anaconda3\\lib\\site-packages (from langchain-core) (0.14.0)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in c:\\users\\gowth\\anaconda3\\lib\\site-packages (from langchain-google-genai) (1.2.0)\n",
      "Requirement already satisfied: google-genai<2.0.0,>=1.56.0 in c:\\users\\gowth\\anaconda3\\lib\\site-packages (from langchain-google-genai) (1.63.0)\n",
      "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in c:\\users\\gowth\\anaconda3\\lib\\site-packages (from google-genai<2.0.0,>=1.56.0->langchain-google-genai) (4.12.1)\n",
      "Requirement already satisfied: google-auth<3.0.0,>=2.47.0 in c:\\users\\gowth\\anaconda3\\lib\\site-packages (from google-auth[requests]<3.0.0,>=2.47.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (2.48.0)\n",
      "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in c:\\users\\gowth\\anaconda3\\lib\\site-packages (from google-genai<2.0.0,>=1.56.0->langchain-google-genai) (0.28.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.28.1 in c:\\users\\gowth\\anaconda3\\lib\\site-packages (from google-genai<2.0.0,>=1.56.0->langchain-google-genai) (2.31.0)\n",
      "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in c:\\users\\gowth\\anaconda3\\lib\\site-packages (from google-genai<2.0.0,>=1.56.0->langchain-google-genai) (15.0.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\gowth\\anaconda3\\lib\\site-packages (from google-genai<2.0.0,>=1.56.0->langchain-google-genai) (1.8.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\gowth\\anaconda3\\lib\\site-packages (from google-genai<2.0.0,>=1.56.0->langchain-google-genai) (1.3.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\gowth\\anaconda3\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core) (2.1)\n",
      "Requirement already satisfied: langgraph-checkpoint<5.0.0,>=2.1.0 in c:\\users\\gowth\\anaconda3\\lib\\site-packages (from langgraph<1.1.0,>=1.0.8->langchain) (4.0.0)\n",
      "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.7 in c:\\users\\gowth\\anaconda3\\lib\\site-packages (from langgraph<1.1.0,>=1.0.8->langchain) (1.0.7)\n",
      "Requirement already satisfied: langgraph-sdk<0.4.0,>=0.3.0 in c:\\users\\gowth\\anaconda3\\lib\\site-packages (from langgraph<1.1.0,>=1.0.8->langchain) (0.3.6)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in c:\\users\\gowth\\anaconda3\\lib\\site-packages (from langgraph<1.1.0,>=1.0.8->langchain) (3.6.0)\n",
      "Requirement already satisfied: orjson>=3.9.14 in c:\\users\\gowth\\anaconda3\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (3.11.7)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\gowth\\anaconda3\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\gowth\\anaconda3\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (0.25.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\gowth\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\gowth\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\gowth\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\gowth\\anaconda3\\lib\\site-packages (from anyio<5.0.0,>=4.8.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (3.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\gowth\\anaconda3\\lib\\site-packages (from google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (0.2.8)\n",
      "Requirement already satisfied: cryptography>=38.0.3 in c:\\users\\gowth\\anaconda3\\lib\\site-packages (from google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (42.0.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\gowth\\anaconda3\\lib\\site-packages (from google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (4.9.1)\n",
      "Requirement already satisfied: certifi in c:\\users\\gowth\\anaconda3\\lib\\site-packages (from httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\gowth\\anaconda3\\lib\\site-packages (from httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\gowth\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (0.16.0)\n",
      "Requirement already satisfied: ormsgpack>=1.12.0 in c:\\users\\gowth\\anaconda3\\lib\\site-packages (from langgraph-checkpoint<5.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.8->langchain) (1.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\gowth\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.28.1->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\gowth\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.28.1->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (2.0.7)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\gowth\\anaconda3\\lib\\site-packages (from cryptography>=38.0.3->google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (1.16.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\gowth\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (0.4.8)\n",
      "Requirement already satisfied: pycparser in c:\\users\\gowth\\anaconda3\\lib\\site-packages (from cffi>=1.12->cryptography>=38.0.3->google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (2.21)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain langchain-core langchain-google-genai python-dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea41ca47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEY = AIzaSyAFNWQCFUQrLE71quKXSDAZzB06OGIqKCg\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(dotenv_path=\".env\", override=True)\n",
    "\n",
    "print(\"KEY =\", os.getenv(\"GOOGLE_API_KEY\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e60e7c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\DBMS\\GenAI_handson\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41558ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the answers for The Matrix:\n",
      "\n",
      "1) The Matrix was released in **1999**.\n",
      "2) As of 2024, that was **25 years ago**.\n"
     ]
    }
   ],
   "source": [
    "# ---------- IMPORTS ----------\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# ðŸ”´ PASTE YOUR REAL KEY HERE\n",
    "GOOGLE_API_KEY = \"AIzaSyAFNWQCFUQrLE71quKXSDAZzB06OGIqKCg\"\n",
    "\n",
    "# ---------- LLM ----------\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    google_api_key=GOOGLE_API_KEY,\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# ---------- ONE-LINE LCEL CHAIN ----------\n",
    "chain = (\n",
    "    ChatPromptTemplate.from_template(\n",
    "        \"Movie: {movie}\\n\"\n",
    "        \"1) Tell me its release year.\\n\"\n",
    "        \"2) Calculate how many years ago that was.\"\n",
    "    )\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# ---------- RUN ----------\n",
    "print(chain.invoke({\"movie\": \"The Matrix\"}))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
